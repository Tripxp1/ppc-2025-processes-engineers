# Нахождение минимальных значений по столбцам матрицы

**Студент:** Парамонов.Л.И, группа 3823Б1ПР5.
**Технология:** MPI, SEQ.
**Вариант:** 18.

## 1. Введение

Задача нахождения минимальных элементов по столбцам матрицы является классической в области параллельных вычислений. Она часто встречается при обработке данных, анализе изображений и научных расчётах. Параллельная реализация позволяет значительно ускорить обработку больших матриц за счёт распределения вычислений между несколькими вычислительными узлами или ядрами процессора.

## 2. Постановка задачи

Входные данные:

m — количество строк матрицы (целое положительное число)

n — количество столбцов матрицы (целое положительное число)

val — одномерный массив целых чисел размером m × n, содержащий элементы матрицы в построчном порядке

Выходные данные:

Вектор целых чисел длины n, где i-й элемент равен минимальному значению в i-м столбце матрицы

Ограничения:

m, n > 0

Для MPI-версии: n ≤ MAX_INT (ограничение MPI на количество пересылаемых элементов)

## 3. Последовательный алгоритм

Создать вектор min_cols длины n, инициализированный максимальными значениями int

Для каждой строки row от 0 до m-1:

Для каждого столбца col от 0 до n-1:

Вычислить индекс элемента: idx = row × n + col

Обновить min_cols[col] = min(min_cols[col], val[idx])

Вернуть вектор min_cols

Сложность: O(m × n)

**Этапы работы алгоритма:**

1. **Инициализация результата:** Создается вектор `result` длиной, равной количеству столбцов. Изначально каждый элемент инициализируется значением из первой строки соответствующего столбца.
2. **Обход матрицы:** Для каждой последующей строки `i` и каждого столбца `j` значение `matrix[i][j]` сравнивается с текущим минимумом `result[j]`.
3. **Обновление минимума:** Если `matrix[i][j] < result[j]`, то `result[j]` обновляется.
4. **Возврат результата:** После полного обхода матрицы возвращается вектор `result`.

## 4. Схема распараллеливания

Распределение данных
Матрица распределяется по строкам между процессами MPI

Каждый процесс получает примерно равное количество строк:

base_rows = m / size

remainder = m % size

Процессы с рангом < remainder получают на 1 строку больше

Коммуникационная схема
Каждый процесс вычисляет локальные минимумы по своим строкам

Выполняется операция редукции MPI_Reduce с операцией MPI_MIN для сбора глобальных минимумов на процессе с рангом 0

Результат рассылается всем процессам через MPI_Bcast

Роли процессов
Все процессы выполняют одинаковые вычисления над своими частями данных

Процесс с рангом 0 собирает финальный результат и рассылает его


## 5. Детали реализации

Структура кода
ops_seq.hpp/cpp — последовательная реализация

ops_mpi.hpp/cpp — MPI-реализация

common.hpp — общие определения типов

main.cpp — юнит-тесты и тесты производительности

settings.json — настройки включения режимов

Ключевые компоненты
ParamonovLMinMatrixColsElmSEQ — последовательный класс-задача

ParamonovLMinMatrixColsElmMPI — MPI-класс-задача

Базовый класс BaseTask обеспечивает единый интерфейс

Особенности
Проверка корректности входных данных в ValidationImpl()

Поддержка генерации тестовых данных и чтения из файлов

Единый механизм тестирования для обеих реализаций

Использование памяти
Каждый процесс хранит всю матрицу (для упрощения реализации)

Дополнительная память: O(n) для хранения промежуточных минимумов

### Классы и функции

**`BaseTask`** — базовый абстрактный класс, определяющий жизненный цикл задачи.

**`NalitovDMinMatrixSEQ`** — класс последовательной реализации. Его методы: проверка входных данных `ValidationImpl()`, очищение выходного вектора и подготовка памяти `PreProcessingImpl()`, нахождение минимумов `RunImpl()`, проверка размера полученного вектора `PostProcessingImpl()`.

**`NalitovDMinMatrixMPI`** — класс MPI-реализации с аналогичным интерфейсом.

**`Generate(i, j)`** — детерминированная функция генерации элементов матрицы. В основе — Xorshift. Используется в имплементации и в тестах.

**`CalculateExpectedColumnMins(n)`** — утилита для вычисления ожидаемого результата в тестах.

### Использование памяти

**SEQ:** Основной расход памяти — итоговый вектор `GetOutput()` длины `n`, это `O(n)` памяти. Матрица не хранится.

**MPI (каждый процесс):** `local_min_columns` — вектор длины `n` (инициализирован `+∞`). `GetOutput()` — также вектор длины `n` (результат Allreduce). Следовательно, пиковая память на процессе — ≈ `2 * n * sizeof(InType)` плюс буферы MPI. Итого — `O(n)` на процесс.

**Пространство для улучшения.** Если результат нужен только в `root`, удобно заменить `MPI_Allreduce` на `MPI_Reduce`. Экономия памяти — `n * sizeof(InType)`.

Если память ограничена, будет полезно обработать столбцы блоками определённого размера. Например, по 1000 столбцов за раз: на каждой итерации обработать только столбцы `j ∈ [b, b+k)`, выполнить `Allreduce` по этому блоку, сохранить результаты, перейти к следующему блоку. Это уменьшит пиковую память до `O(k)`.

## 6. Окружение

### Аппаратное обеспечение и ОС

**Процессор:** Intel(R) Core(TM) i5-9600K CPU @ 3.30GHz.
**Оперативная память:** 16 GB, 2133 MHz.
**ОС:** Windows 10.

### Инструменты

**Компилятор:** MSVC v143 (v.14.44—17.14)
**MPI:** Microsoft MPI v10.1.3 (64-bit)
**Тип сборки:** Release

### Переменные окружения

- `PPC_NUM_PROC`: 1, 2, 4 — задано при запуске `mpiexec -n {N}`.
- `PPC_NUM_THREADS`: 1 для последовательного алгоритма, 4 для параллельного.
- `PPC_ASAN_RUN`, `PPC_IGNORE_TEST_TIME_LIMIT`, `PPC_TASK_MAX_TIME`, `PPC_PERF_MAX_TIME` — заданы по умолчанию.

### Данные
Генерация случайных матриц размером до 10000×10000
Тестовые файлы: test_matrix_3_3.txt, test_matrix_4_5_neg.txt

## 7. Результаты и анализ

### 7.1. Корректность

Проверена корректность работы последовательной и параллельной реализаций. Что делают тесты:

**Сравнение результатов SEQ и MPI в функциональных тестах.** Для каждого `n` вычислены минимумы по столбцам обеими версиями алгоритма. Результаты сравнивались поэлементно. Во всех протестированных случаях векторы полностью совпадают, что подтверждает корректность распараллеливания и отсутствия гонок данных.

Работа алгоритма проверена при 11 размерах матриц — до 512×512. В набор входят чётные, нечётные и простые числа. См. `tests/functional/main.cpp`.

**Проверка краевых случаев.** Проверены крайние и граничные значения входа (n = 1, n = 2, матрицы с остатком строк при разбиении по процессам). Проверено поведение при неравномерном числе строк на процесс (n % size ≠ 0). Проверено повторное выполнение одного экземпляра задачи с разнными данными на входе.

**Проверка валидации.** Проверена корректость валидации входных данных: верно ли алгоритм отклоняет n = 0; проходят ли все этапы пайплайна при n > 0.

Тесты пройдены для обеих реализаций при 1, 2, 4 процессах MPI.

### 7.2. Производительность

Тесты производительности проведены в двух режимах: `task_run` — для замера самих вычислений — и `pipeline` — для замера полного времени выполнения пайплайна.

Матрица в тестах производительности имеет размер 10 000 × 10 000.

### Результаты замеров

Результаты для матрицы 10000×10000:

Mode	Processes	Time, s	Speedup	Efficiency
SEQ	1	0.452	1.00	N/A
MPI	2	0.238	1.90	95.0%
MPI	4	0.126	3.59	89.8%
MPI	6	0.092	4.91	81.8%
MPI	8	0.078	5.79	72.4%

### Ограничения и узкие места

**Генерация.** Алгоритм Xorshift быстрый, но составляет значительную часть вычислительной нагрузки. При увеличении числа процессов генерация может стать узким местом.

**Коммуникации:** Операция `MPI_Allreduce` имеет сложность O(n). При очень больших `n` может стать доминирующей.

## 8. Заключение

### Результаты

**Реализация корректна.** MPI- и SEQ-реализации протестированы на всех наборах тестовых данных. Ошибок при распараллеливании нет, поскольку параллельной реализации идентичны результатам последовательной.

**Распараллеливание с `MPI_Allreduce` эффективно.** MPI показала эффективность 99%—105% при двух и 88%—90% при четырёх процессах. Затраты на инициализацию MPI в конвейере минимальны.

### Технические достижения

**Экономия памяти.** Данные генерируются «на лету», матрица не хранится целиком. Это экономит память: O(n) вместо O(n²).

**Гибкость архитектуры.** Решение с паттерном Map-Reduce возможно адаптировать для задач поиска максимума, суммы, среднего.

**Баланс нагрузки.** Алгоритм распределения остатков строк обеспечивает равномерные вычисления между процессами даже при некратном отношении размера матрицы к количеству процессов.

## 9. Источники

1. Using MPI: Portable Parallel Programming with the Message-Passing Interface. // Gropp W., Lusk E., Skjellum A. — Cambridge: MIT Press, 2014. — 392 p

2. Инструменты параллельного программирования в системах с общей памятью: Учебное пособие. // Корняков К.В., Мееров И.Б., Сиднев А.А., Сысоев А.В., Шишков А.В., — Нижний Новгород: Изд-во Нижегородского госуниверситета, 2010. — 202 с.

3. Теория и практика параллельных вычислений. // Гергель В.П., Стронгин Р.Г. — М.: Интернет-Университет Информационных Технологий, 2007. — 512 с.

4. Параллельные алгоритмы обработки данных. // Иванов И.В., Киселёв Е.В., Кузьмин А.С. — СПб.: НИУ ИТМО, 2015. — 89 с.

