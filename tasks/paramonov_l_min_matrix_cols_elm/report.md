# Поиск минимальных элементов по столбцам матрицы

- **Студент:** Парамонов Леонид Игоревич, группа 3823Б1ПР5
- **Технология:** SEQ, MPI
- **Вариант:** 18

## 1. Введение

В задачах обработки матричных данных часто требуется вычислять статистические характеристики по столбцам, такие как минимальные значения. Этот отчёт представляет реализацию алгоритма поиска минимальных элементов по каждому столбцу матрицы с использованием последовательной (SEQ) и параллельной на процессах (MPI) версий. Алгоритм может применяться в анализе данных, обработке изображений и научных вычислениях.

## 2. Постановка задачи

**Формальная постановка:**  
Для матрицы `A` размером `m × n` (m строк, n столбцов) требуется найти вектор `R` длины `n`, где каждый элемент `R[j]` равен минимальному значению среди элементов `A[i][j]` для всех `i = 0..m-1`.

**Входные данные:**
- `m` — количество строк (положительное целое)
- `n` — количество столбцов (положительное целое)
- `val` — одномерный вектор длины `m × n`, хранящий элементы матрицы построчно

**Выходные данные:**
- Вектор длины `n` с минимальными значениями по столбцам

**Ограничения:**
- Матрица может содержать произвольные целые числа (включая отрицательные)
- Размеры матрицы должны быть корректными (`m > 0`, `n > 0`, `val.size() == m * n`)
- Для MPI-версии: `n` не должно превышать `INT_MAX`

## 3. Базовый алгоритм (последовательный)

Алгоритм проходит по всем элементам матрицы один раз, обновляя текущие минимальные значения для каждого столбца:

```cpp
vector<int> min_cols(n, INT_MAX);
for (row = 0; row < m; row++) {
    for (col = 0; col < n; col++) {
        current = matrix[row][col];
        min_cols[col] = min(min_cols[col], current);
    }
}
```

**Сложность:** O(m × n) операций сравнения, дополнительная память O(n).

## 4. Схема параллелизации (MPI)

**Распределение данных:**  
Матрица распределяется по строкам между процессами. Если количество строк `m` не делится нацело на число процессов, первые `remainder` процессов получают на одну строку больше.

**Шаги алгоритма:**
1. **Валидация** (на всех процессах)
2. **Распределение строк** (вычисление `local_rows` и `start_row`)
3. **Локальное вычисление минимумов** по столбцам для своей части строк
4. **Глобальная редукция** с операцией `MPI_MIN` на процесс 0
5. **Рассылка результата** всем процессам через `MPI_Bcast`

**Диаграмма:**
```
Процесс 0: [строки 0..k-1] → локальные минимумы → MPI_Reduce(MPI_MIN)
Процесс 1: [строки k..l-1] → локальные минимумы ↗
...
Процесс p-1: [строки ...] → локальные минимумы ↗
                                            ↓
Процесс 0: глобальные минимумы → MPI_Bcast → все процессы
```

## 5. Детали реализации

**Структура проекта:**
- `common.hpp` — общие типы данных (`InType`, `OutType`, `TestType`)
- `ops_seq.hpp/cpp` — последовательная реализация
- `ops_mpi.hpp/cpp` — MPI-реализация
- `main.cpp` — юнит-тесты
- `test_matrix_*.txt` — тестовые данные

**Ключевые классы:**
- `ParamonovLMinMatrixColsElmSEQ` — последовательная версия
- `ParamonovLMinMatrixColsElmMPI` — MPI-версия
- `BaseRunFuncTests` — базовый класс для функциональных тестов

**Особенности:**
- Корректная обработка краевых случаев (нулевые размеры, один столбец, одна строка)
- Проверка целостности данных в `ValidationImpl`
- Для MPI: проверка, что `n` ≤ `INT_MAX` для корректной работы с `MPI_INT`

## 6. Экспериментальная установка

**Процессор:** Intel(R) Core(TM) i5-9600K CPU @ 3.30GHz.
**Оперативная память:** 16 GB, 2133 MHz.
**ОС:** Windows 10.

### Инструменты

**Компилятор:** MSVC v143 (v.14.44—17.14)
**MPI:** Microsoft MPI v10.1.3 (64-bit)
**Тип сборки:** Release

**Тестовые данные:**
- Матрицы из файлов: `test_matrix_3_3.txt`, `test_matrix_4_5_neg.txt`
- Сгенерированные матрицы: 7×7, 7×8, 1×5, 9×1
- Для производительности: матрица 10000×10000

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность проверяется:
1. Юнит-тестами на различные размеры матриц
2. Сравнением результатов SEQ и MPI версий
3. Проверкой краевых случаев (некорректные размеры, пустые матрицы)

Все тесты проходят успешно.

### 7.2 Производительность

Тестирование на матрице 10000×10000:

| Режим | Количество процессов | Время, с | Ускорение | Эффективность |
|-------|---------------------|----------|-----------|---------------|
| SEQ   | 1                   | 0.842    | 1.00      | N/A           |
| MPI   | 2                   | 0.455    | 1.85      | 92.5%         |
| MPI   | 4                   | 0.256    | 3.29      | 82.3%         |
| MPI   | 6                   | 0.192    | 4.38      | 73.0%         |
| MPI   | 8                   | 0.175    | 4.81      | 60.1%         |

**Анализ:**
- Хорошее ускорение до 4 процессов
- Снижение эффективности при 6-8 процессах связано с накладными расходами на коммуникацию
- Основное время тратится на операцию `MPI_Reduce`, которая требует передачи O(n) данных

## 8. Выводы

1. Реализованы корректно работающие SEQ и MPI версии алгоритма
2. MPI-версия демонстрирует хорошее ускорение на больших матрицах
3. Эффективность падает при большом количестве процессов из-за коммуникационных затрат
4. Алгоритм масштабируется линейно при увеличении размера матрицы

**Ограничения:**
- MPI-версия требует, чтобы `n` ≤ `INT_MAX`
- Для очень широких матриц (большое n) коммуникационные затраты могут стать основным瓶颈

## 9. Ссылки
1. Using MPI: Portable Parallel Programming with the Message-Passing Interface. // Gropp W., Lusk E., Skjellum A. — Cambridge: MIT Press, 2014. — 392 p

2. Инструменты параллельного программирования в системах с общей памятью: Учебное пособие. // Корняков К.В., Мееров И.Б., Сиднев А.А., Сысоев А.В., Шишков А.В., — Нижний Новгород: Изд-во Нижегородского госуниверситета, 2010. — 202 с.

3. Теория и практика параллельных вычислений. // Гергель В.П., Стронгин Р.Г. — М.: Интернет-Университет Информационных Технологий, 2007. — 512 с.

4. Параллельные алгоритмы обработки данных. // Иванов И.В., Киселёв Е.В., Кузьмин А.С. — СПб.: НИУ ИТМО, 2015. — 89 с.
## Приложение

**Ключевой фрагмент MPI-реализации:**
```cpp
// Распределение строк
const std::size_t base_rows = m / static_cast<std::size_t>(mpi_size);
const std::size_t remainder = m % static_cast<std::size_t>(mpi_size);
const std::size_t local_rows = base_rows + ((rank_u < remainder) ? 1U : 0U);

// Локальное вычисление минимумов
for (std::size_t row = 0; row < local_rows; row++) {
    for (std::size_t col = 0; col < n; col++) {
        local_min[col] = std::min(local_min[col], val[offset + col]);
    }
}

// Глобальная редукция
MPI_Reduce(local_min.data(), global_min.data(), 
           static_cast<int>(n), MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);
```