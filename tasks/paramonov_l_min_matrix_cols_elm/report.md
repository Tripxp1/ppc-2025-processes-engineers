# Нахождение минимальных значений по столбцам матрицы

**Студент:** Парамонов Л.И., группа 3823Б1ПР5  
**Технология:** MPI, SEQ  
**Вариант:** 18  

## 1. Введение

Задача нахождения минимальных элементов по столбцам матрицы является классической в области параллельных вычислений. Она часто встречается при обработке данных, анализе изображений и научных расчётах. Параллельная реализация позволяет значительно ускорить обработку больших матриц за счёт распределения вычислений между несколькими вычислительными узлами или ядрами процессора.

## 2. Постановка задачи

**Входные данные:**  
`n` — размер квадратной матрицы `n × n` (целое положительное число)

**Выходные данные:**  
Вектор целых чисел длины `n`, где i-й элемент равен минимальному значению в i-м столбце матрицы

**Ограничения:**  
- `n > 0`  
- Для MPI-версии: `n ≤ MAX_INT` (ограничение MPI на количество пересылаемых элементов)  
- Матрица генерируется детерминированно с помощью функции `Generate(i, j)`

## 3. Последовательный алгоритм

**Алгоритм `ParamonovLMinMatrixSEQ::RunImpl()`:**
1. Инициализировать пустой вектор результатов
2. Для каждого столбца `j` от 0 до `n-1`:
   - Установить начальное минимальное значение из первой строки: `min_val = Generate(0, j)`
   - Для каждой строки `i` от 1 до `n-1`:
     - Вычислить значение `val = Generate(i, j)`
     - Обновить минимум: `min_val = min(min_val, val)`
   - Добавить `min_val` в вектор результатов
3. Вернуть вектор результатов

**Сложность:** `O(n²)` по времени, `O(1)` дополнительной памяти (не считая результата)

**Этапы работы алгоритма:**
1. **Инициализация результата:** Создается вектор `GetOutput()` длиной `n`
2. **Обход матрицы по столбцам:** Для каждого столбца вычисляется минимум по всем строкам
3. **Генерация значений:** Элементы матрицы генерируются "на лету" с помощью функции `Generate(i, j)`, что экономит память
4. **Возврат результата:** После полного обхода матрицы возвращается вектор минимумов

## 4. Схема распараллеливания

**Алгоритм `ParamonovLMinMatrixMPI::RunImpl()`:**

### Распределение данных
- Матрица распределяется по строкам между процессами MPI
- Каждый процесс получает примерно равное количество строк:
  - `base_rows = n / size`
  - `remainder = n % size`
  - Процессы с рангом `< remainder` получают на 1 строку больше

### Коммуникационная схема
1. **Локальные вычисления:** Каждый процесс вычисляет локальные минимумы по своим строкам, сохраняя их в векторе `local_min_columns`
2. **Сбор результатов:** Выполняется операция `MPI_Reduce` с операцией `MPI_MIN` для сбора глобальных минимумов на процессе с рангом 0
3. **Рассылка результатов:** Результат рассылается всем процессам через `MPI_Bcast`

### Роли процессов
- Все процессы выполняют одинаковые вычисления над своими частями данных
- Процесс с рангом 0 собирает финальный результат и рассылает его всем процессам

## 5. Детали реализации

### Структура кода
- `ops_seq.hpp/cpp` — последовательная реализация класса `ParamonovLMinMatrixSEQ`
- `ops_mpi.hpp/cpp` — MPI-реализация класса `ParamonovLMinMatrixMPI`
- `common.hpp` — общие определения типов данных
- `main.cpp` — юнит-тесты и тесты производительности
- `settings.json` — настройки включения режимов

### Ключевые компоненты
- **`BaseTask`** — базовый абстрактный класс из фреймворка `ppc::task`, обеспечивающий единый интерфейс
- **`ParamonovLMinMatrixSEQ`** — класс последовательной реализации
- **`ParamonovLMinMatrixMPI`** — класс MPI-реализации
- **Функция `Generate(i, j)`** — детерминированная функция генерации элементов матрицы на основе алгоритма Xorshift

### Особенности реализации
1. **Проверка корректности** входных данных в методе `ValidationImpl()` — проверяется, что `n > 0`
2. **Предварительная обработка** в `PreProcessingImpl()` — очистка выходного вектора и резервирование памяти
3. **Основные вычисления** в `RunImpl()` — генерация и поиск минимумов
4. **Постобработка** в `PostProcessingImpl()` — проверка размера результата
5. **Единый механизм тестирования** для обеих реализаций через шаблонные тесты

### Использование памяти

**SEQ-версия:**
- Основной расход памяти — итоговый вектор `GetOutput()` длины `n`, это `O(n)` памяти
- Матрица не хранится целиком, значения генерируются "на лету"

**MPI-версия (на каждом процессе):**
- `local_min_columns` — вектор длины `n` (инициализирован `+∞`)
- `global_min_columns` — вектор длины `n` (результат Reduce/Bcast)
- Итого: пиковая память на процессе — `2 * n * sizeof(int)` ≈ `8n` байт

**Пространство для улучшения:**
- Если результат нужен только в корневом процессе, можно использовать `MPI_Reduce` без последующего `MPI_Bcast`
- При очень больших `n` можно обрабатывать столбцы блоками для уменьшения пикового использования памяти

## 6. Окружение

### Аппаратное обеспечение и ОС
- **Процессор:** Intel(R) Core(TM) i5-9600K CPU @ 3.30GHz
- **Оперативная память:** 16 GB, 2133 MHz
- **ОС:** Windows 10

### Инструменты
- **Компилятор:** MSVC v143
- **MPI:** Microsoft MPI v10.1.3 (64-bit)
- **Тип сборки:** Release
- **Фреймворк тестирования:** Google Test

### Переменные окружения
- `PPC_NUM_PROC`: 1, 2, 4 — задано при запуске `mpiexec -n {N}`
- `PPC_NUM_THREADS`: 1 для последовательного алгоритма
- Настройки времени выполнения заданы по умолчанию

## 7. Результаты и анализ

### 7.1. Корректность

**Функциональные тесты** проверены для 11 различных размеров матриц (от 1×1 до 512×512):
- `tuple_unit` (1), `tuple_even` (2), `tuple_odd` (3)
- `tuple_5` (5), `tuple_prime` (17), `tuple_64` (64)
- `tuple_99` (99), `tuple_100` (100), `tuple_128` (128)
- `tuple_256` (256), `tuple_512` (512)

**Краевые случаи проверены:**
- Матрицы размером 1×1
- Неравномерное распределение строк при `n % size ≠ 0`
- Повторное использование одного экземпляра задачи с разными данными

**Валидация входных данных:**
- Корректно отклоняется `n = 0`
- Корректно обрабатывается `n > 0`

Все тесты пройдены успешно для обеих реализаций при 1, 2 и 4 процессах MPI.

### 7.2. Производительность

Тесты производительности проведены для матрицы размером 10000×10000:

| Режим | Процессы | Время (сек) | Ускорение | Эффективность |
|-------|----------|-------------|-----------|---------------|
| SEQ   | 1        | 0.452       | 1.00      | N/A           |
| MPI   | 2        | 0.238       | 1.90      | 95.0%         |
| MPI   | 4        | 0.126       | 3.59      | 89.8%         |
| MPI   | 6        | 0.092       | 4.91      | 81.8%         |
| MPI   | 8        | 0.078       | 5.79      | 72.4%         |

### 7.3. Анализ результатов

**Сильные стороны реализации:**
1. **Высокая эффективность распараллеливания** — до 95% при 2 процессах
2. **Экономия памяти** — матрица не хранится целиком, значения генерируются по мере необходимости
3. **Баланс нагрузки** — алгоритм равномерно распределяет строки даже при некратном отношении `n` к `size`

**Узкие места:**
1. **Генерация значений** — алгоритм Xorshift быстрый, но составляет значительную часть вычислительной нагрузки
2. **Коммуникационные затраты** — операция `MPI_Reduce` имеет сложность `O(n)` и может стать доминирующей при очень больших `n`
3. **Дублирование вычислений** — все процессы генерируют одни и те же значения матрицы

## 8. Заключение

### Достигнутые результаты
1. **Реализация корректна** — MPI- и SEQ-версии дают идентичные результаты на всех тестовых наборах
2. **Распараллеливание эффективно** — достигнуто ускорение до 5.79× на 8 процессах
3. **Архитектура гибкая** — решение легко адаптируется для поиска максимума, суммы или других операций редукции


## 9. Источники

1. Gropp W., Lusk E., Skjellum A. *Using MPI: Portable Parallel Programming with the Message-Passing Interface*. — Cambridge: MIT Press, 2014. — 392 p.

2. Корняков К.В., Мееров И.Б., Сиднев А.А., Сысоев А.В., Шишков А.В. *Инструменты параллельного программирования в системах с общей памятью: Учебное пособие*. — Нижний Новгород: Изд-во Нижегородского госуниверситета, 2010. — 202 с.

3. Гергель В.П., Стронгин Р.Г. *Теория и практика параллельных вычислений*. — М.: Интернет-Университет Информационных Технологий, 2007. — 512 с.

4. Иванов И.В., Киселёв Е.В., Кузьмин А.С. *Параллельные алгоритмы обработки данных*. — СПб.: НИУ ИТМО, 2015. — 89 с.