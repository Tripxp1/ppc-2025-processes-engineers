# Передача от одного всем (broadcast)

- **Студент:** Парамонов Леонид Игоревич, группа 3823Б1ПР5
- **Технология:** SEQ, MPI
- **Вариант:** 1

## Введение  
В параллельных вычислениях широко используется операция рассылки данных от одного процесса (корневого) всем остальным участникам коммуникационной группы. Эта операция является одной из базовых коллективных коммуникаций в стандарте MPI. В данной работе реализована операция broadcast для произвольного числового буфера (целые числа, числа с плавающей запятой одинарной и двойной точности) с использованием библиотеки MPI, а также последовательная версия для проверки корректности. Цель работы — исследовать эффективность реализации broadcast на основе древовидного алгоритма и сравнить её с использованием штатной функции `MPI_Bcast`.

## Постановка задачи  
Требуется разработать и реализовать алгоритм рассылки данных от одного процесса всем остальным, удовлетворяющий следующим условиям:
- Поддержка трёх типов данных: `int`, `float`, `double`.
- Работа с буфером произвольного размера (в пределах, поддерживаемых MPI).
- Корректная обработка некорректных входных данных (например, отрицательный корень, пустой буфер).
- Обеспечение корректности рассылки при любом количестве процессов и любом назначении корневого процесса.
- Оценка производительности и сравнение с реализацией на основе `MPI_Bcast`.

Ограничения:
- Реализация должна быть совместима со стандартом MPI.
- Необходимо обеспечить thread-safety в рамках одного процесса.
- Размер данных должен помещаться в тип `int` (ограничение MPI).

## Описание алгоритма (базового/последовательного)  
В последовательной версии (`ParamonovFromOneToAllSEQ`) операция broadcast тривиальна: корневой процесс уже содержит все данные, поэтому результат — просто копия входного буфера. Проверяются только корректность входных данных (непустой буфер, неотрицательный корень, согласованность типа).

## Схема распараллеливания  
В MPI-реализации (`ParamonovFromOneToAllMPI`) используется **древовидный алгоритм** с шагом, удваивающимся на каждой итерации:
1. **Трансляция рангов**: все ранги преобразуются в «логические» так, чтобы корневой процесс имел логический ранг 0.
2. **Рассылка размера данных**: сначала от корня всем процессам рассылается размер буфера с использованием того же древовидного алгоритма.
3. **Рассылка самих данных**: данные передаются по тому же дереву:
   - На каждом шаге процессы с логическим рангом `< step` отправляют данные процессам с рангом `rank + step`.
   - Процессы с логическим рангом `≥ step` и `< 2*step` получают данные от процесса с рангом `rank − step`.
4. Используются стандартные MPI-функции `MPI_Send` и `MPI_Recv`.

Такой подход уменьшает общее количество коммуникационных операций по сравнению с линейной рассылкой и позволяет эффективно использовать сеть.

## Экспериментальные результаты  

**Процессор:** Intel(R) Core(TM) i5-9600K CPU @ 3.30GHz.
**Оперативная память:** 16 GB, 2133 MHz.
**ОС:** Windows 10.

### Инструменты

**Компилятор:** MSVC v143 (v.14.44—17.14)
**MPI:** Microsoft MPI v10.1.3 (64-bit)
**Тип сборки:** Release 

### Таблицы времени выполнения (в мс)  
| Количество процессов | Древовидный алгоритм | MPI_Bcast |
|----------------------|----------------------|-----------|
| 2                    | 0.12                 | 0.10      |
| 4                    | 0.18                 | 0.15      |
| 8                    | 0.25                 | 0.21      |
| 12                   | 0.32                 | 0.28      |

Размер данных: 50 000 целых чисел (200 КБ).

### Ускорение и эффективность  
- Ускорение по сравнению с линейной рассылкой (не реализована) — примерно в log₂(P) раз.
- Эффективность древовидного алгоритма относительно `MPI_Bcast`: 85–90% (потери связаны с накладными расходами на организацию дерева).

### Проверка корректности  
Корректность проверена:
1. **Юнит-тестами** (gtest) для последовательной версии.
2. **Функциональными тестами** для MPI-версии с различными типами данных, размерами буфера и корневыми процессами.
3. **Perf-тестами** на больших данных (50 000 элементов).
4. Проверка на некорректных входных данных (отрицательный корень, пустой буфер) возвращает ошибку.

## Вывод  
- **Что сработало**: древовидный алгоритм эффективно распараллеливает broadcast, обеспечивая логарифмическое количество шагов. Реализация корректно работает с разными типами данных и размерами буфера.
- **Что можно улучшить**: 
  - Добавить поддержку пользовательских типов данных (через MPI_Type_create_struct).
  - Реализовать гибридный алгоритм (например, binomial tree + scatter) для очень больших данных.
  - Оптимизировать выбор шага дерева в зависимости от размера сообщения и топологии сети.

## Источники  
1.  Инструменты параллельного программирования в системах с общей памятью: Учебное пособие. // Корняков К.В., Мееров И.Б., Сиднев А.А., Сысоев А.В., Шишков А.В., — Нижний Новгород: Изд-во Нижегородского госуниверситета, 2010. — 202 с.  
2. Теория и практика параллельных вычислений. // Гергель В.П., Стронгин Р.Г. — М.: Интернет-Университет Информационных Технологий, 2007.  
3. OpenMPI documentation: https://www.open-mpi.org/doc/  

## Appendix (код)

```cpp
template <typename T>
void BroadcastPayload(int world_size, int logical_rank, int root,
                      std::vector<T> &buffer, int count, MPI_Datatype mpi_type) {
  TreeWalk(world_size, logical_rank,
    [&](int step) {
      const int dst_logical = logical_rank + step;
      const int dst_rank = (dst_logical + root) % world_size;
      MPI_Send(buffer.data(), count, mpi_type, dst_rank, 1, MPI_COMM_WORLD);
    },
    [&](int step) {
      const int src_logical = logical_rank - step;
      const int src_rank = (src_logical + root) % world_size;
      MPI_Recv(buffer.data(), count, mpi_type, src_rank, 1,
               MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    });
}
```