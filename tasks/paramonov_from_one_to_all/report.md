# Передача от одного всем (broadcast)

**Студент:** Парамонов Леонид Игоревич, группа 3823Б1ПР5 
**Технологии:** MPI, последовательная реализация  
**Вариант:** 1

## 1. Обзор задачи

Операция широковещательной рассылки, когда один процесс передает данные всем остальным участникам коммуникатора, является фундаментальной для параллельных вычислений. Стандартная функция `MPI_Bcast` использует оптимизированные алгоритмы для эффективной передачи. В данной работе требуется создать собственную реализацию этой операции, используя только базовые средства передачи `MPI_Send` и `MPI_Recv`, а также сравнить ее производительность с последовательной версией.

Основная цель — разработка и анализ параллельного алгоритма рассылки на основе биномиального дерева для массивов целых и вещественных чисел, проверка корректности работы и оценка эффективности.

## 2. Формальная постановка

**Задача:** создать функцию коллективной рассылки данных от выбранного процесса-источника всем процессам в коммуникаторе `MPI_COMM_WORLD`. Реализация должна использовать исключительно точечные операции передачи `MPI_Send` и `MPI_Recv`. Функциональный интерфейс должен соответствовать стандартному `MPI_Bcast`. Требуется поддержка типов данных: `MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE`.

**Входные данные:** структура

```
struct InputData {
  std::variant<std::vector<int>, std::vector<float>, std::vector<double>> buffer;
  int root_rank;
};
```

где `buffer` содержит передаваемые данные только на процессе-источнике (`root_rank`), а на остальных процессах буфер пуст. Параметр `root_rank` может принимать любое значение в диапазоне `[0, количество_процессов)`.

**Выходные данные:** `std::variant` с контейнером того же типа, что и входные данные, заполненный пересланными значениями на всех процессах.

**Требования:**

-   должна использоваться древовидная схема передачи;
-   разрешены только базовые MPI-операции `MPI_Send` и `MPI_Recv` (допускаются вспомогательные функции для получения параметров коммуникатора, размеров типов и т.п.);
-   результаты параллельной и последовательной реализаций должны быть идентичны;
-   тестирование должно проводиться для различных значений `root_rank`.

## 3. Последовательная реализация

Последовательная версия служит эталоном для проверки корректности. Она выполняет прямое копирование данных внутри одного процесса без организации коммуникаций.

**Вычислительная сложность:** `O(n)`, где `n` — количество элементов.  
**Использование памяти:** `O(n)` для хранения результирующего массива.

Основная логика реализации:

```cpp
bool SequentialBroadcast::Execute() {
  try {
    const auto &input = GetInput();
    auto &output = GetOutput();

    if (std::holds_alternative<std::vector<int>>(input.buffer)) {
      return CopyData<int>(input, output);
    }
    if (std::holds_alternative<std::vector<float>>(input.buffer)) {
      return CopyData<float>(input, output);
    }
    if (std::holds_alternative<std::vector<double>>(input.buffer)) {
      return CopyData<double>(input, output);
    }

    return false;
  } catch (...) {
    return false;
  }
}
```

Обработка данных следует стандартному пайплайну: проверка входных параметров (`ValidationImpl`), подготовка выходных структур (`PreProcessingImpl`), копирование данных (`RunImpl`) и финализация (`PostProcessingImpl`).

## 4. Параллельный алгоритм

### Основная концепция

Для минимизации времени передачи используется биномиальное дерево. Процессы, уже получившие данные, передают их другим участникам, чьи виртуальные номера отличаются на возрастающие степени двойки. Это позволяет сократить количество шагов передачи с линейного до логарифмического относительно числа процессов.

### Преобразование рангов

Для работы с произвольным процессом-источником выполняется переход к виртуальной нумерации:

```
virtual_rank = (actual_rank - root_rank + total_processes) % total_processes
```

В этой системе источник имеет виртуальный ранг 0, что унифицирует построение дерева передачи. На каждом шаге маска `mask = 1, 2, 4, ...` определяет пары процессов для обмена.

### Алгоритм передачи

```pascal
function CustomBroadcast(buffer, count, datatype, root_rank, comm): int
  total := MPI_Comm_size(comm)
  rank := MPI_Comm_rank(comm)
  virtual_rank := (rank - root_rank + total) mod total
  mask := 1
  while mask < total do
    if virtual_rank < mask then
      dest_virtual := virtual_rank + mask
      if dest_virtual < total then
        dest_rank := (dest_virtual + root_rank) mod total
        MPI_Send(buffer, count, datatype, dest_rank, TAG, comm)
    else if virtual_rank < 2 * mask then
      src_virtual := virtual_rank - mask
      src_rank := (src_virtual + root_rank) mod total
      MPI_Recv(buffer, count, datatype, src_rank, TAG, comm, status)
    mask := mask * 2
  return MPI_SUCCESS
```

### Передача служебной информации

Перед рассылкой основных данных процесс 0 (не обязательно являющийся источником) распространяет метаданные: тип данных, размер массива. Для этого используется та же реализация `CustomBroadcast`.

### Оценка сложности

-   **Коммуникационная сложность:** `O(log p)` шагов, где `p` — количество процессов.
-   **Вычислительная сложность:** операции копирования данных — `O(n)` на процесс-источник и на каждом шаге объединения.
-   **Потребление памяти:** каждый процесс хранит полную копию массива размером `O(n)`.

## 5. Особенности реализации

### Структура проекта

```
broadcast_implementation/
├── common/include/common_types.hpp   # Общие структуры данных
├── sequential/include/seq_ops.hpp    # Интерфейс последовательной версии
├── sequential/src/seq_ops.cpp        # Реализация последовательного копирования
├── parallel/include/mpi_ops.hpp      # Объявление параллельных функций
├── parallel/src/mpi_ops.cpp          # Реализация биномиального дерева
└── tests/functional/test_main.cpp    # Набор тестов на базе Google Test
```

### Основные компоненты

1.  **`int CustomBroadcast(void* buffer, int count, MPI_Datatype datatype, int root_rank, MPI_Comm comm)`** — основная функция рассылки. Возвращает коды ошибок MPI при некорректных параметрах.
2.  **`template <typename T> bool DistributeVector(...)`** — метод класса `ParallelBroadcast`, подготавливающий буферы и вызывающий `CustomBroadcast` для конкретного типа данных.
3.  **`BroadcastMetadata`** — вспомогательная функция для рассылки скалярных значений (ранг источника, размер массива) тем же алгоритмом.

Код шага биномиального дерева:

```cpp
int TreeTransmissionStep(void *data_buffer, int element_count,
                         MPI_Datatype data_type, int source_rank,
                         int virtual_rank, int current_mask,
                         int process_count, MPI_Comm communicator) {
  if (virtual_rank < current_mask) {
    const int receiver_virtual = virtual_rank + current_mask;
    if (receiver_virtual >= process_count) {
      return MPI_SUCCESS;
    }
    const int receiver_actual = (receiver_virtual + source_rank) % process_count;
    return MPI_Send(data_buffer, element_count, data_type,
                    receiver_actual, TRANSMISSION_TAG, communicator);
  }

  if (virtual_rank < (current_mask * 2)) {
    const int sender_virtual = virtual_rank - current_mask;
    const int sender_actual = (sender_virtual + source_rank) % process_count;
    return MPI_Recv(data_buffer, element_count, data_type,
                    sender_actual, TRANSMISSION_TAG,
                    communicator, MPI_STATUS_IGNORE);
  }

  return MPI_SUCCESS;
}
```

### Проверка корректности и обработка ошибок

-   Выполняется валидация входных параметров: неотрицательный размер данных, допустимый ранг источника, корректный тип данных.
-   При нулевом размере массива функция сразу возвращает `MPI_SUCCESS`, что соответствует спецификации MPI.
-   Все коммуникации используют фиксированный тег `TRANSMISSION_TAG`, так как на каждом этапе каждый процесс ожидает данные строго от одного отправителя.

### Тестирование

Тестовый модуль `test_main.cpp` включает 16 различных сценариев: массивы положительных и отрицательных чисел, смешанные значения, пустые массивы и большие наборы (1000 элементов) для каждого поддерживаемого типа. Ранг источника циклически меняется по формуле `source_rank = test_index % world_size`, что обеспечивает проверку всех возможных источников. Для каждого теста сравниваются результаты последовательной и параллельной реализаций.

## 6. Конфигурация выполнения

### Аппаратное обеспечение и ОС

Процессор: Intel(R) Core(TM) i5-9600K CPU @ 3.30GHz.
Оперативная память: 16 GB, 2133 MHz.
ОС: Windows 10.

### Инструменты

Компилятор: MSVC v143 (v.14.44—17.14)
MPI: Microsoft MPI v10.1.3 (64-bit)
Тип сборки: Release

### Параметры запуска

-   `PPC_NUM_PROC`: 1, 2, 4 (задается через `mpiexec -n {N}`)
-   `PPC_NUM_THREADS`: 1 для последовательного варианта, 4 для параллельного
-   Параметры `PPC_ASAN_RUN`, `PPC_IGNORE_TEST_TIME_LIMIT`, `PPC_TASK_MAX_TIME`, `PPC_PERF_MAX_TIME` установлены по умолчанию

### Данные для тестирования

-   Размеры массивов в функциональных тестах: от 0 до 1000 элементов
-   Для оценки производительности используются массивы по 100 000 элементов каждого типа, заполненные детерминированными последовательностями

## 7. Результаты эксперимента

### 7.1. Проверка корректности

-   Все функциональные тесты пройдены для 1, 2 и 4 процессов. Результаты параллельной и последовательной версий полностью совпадают.
-   Алгоритм корректно работает с любым процессом-источником благодаря циклическому выбору `source_rank` в тестах.
-   Обработаны граничные случаи: пустые массивы, единичные элементы, некратные размеры данных.
-   Валидация входных параметров предотвращает выполнение при некорректных данных.

### 7.2. Оценка производительности

Измерения проводились в двух режимах: `task_run` (время непосредственно вычислений) и `pipeline` (полное время выполнения). Размер массива в тестах производительности — 100 000 элементов.

#### Результаты измерений

**Последовательная версия (task_run)**  
Время: 0.002072 с · Ускорение: 1.00

**Параллельная версия, 2 процесса (task_run)**  
Время: 0.006360 с · Ускорение: 0.33 · Эффективность: 16.5%

**Последовательная версия (pipeline)**  
Время: 0.002098 с · Ускорение: 1.00

**Параллельная версия, 2 процесса (pipeline)**  
Время: 0.006205 с · Ускорение: 0.34 · Эффективность: 17.0%

#### Анализ результатов

-   Для массивов умеренного размера последовательная версия оказывается быстрее из-за накладных расходов на организацию MPI-коммуникаций.
-   Коэффициент ускорения меньше единицы — ожидаемый результат при малом объеме данных и небольшом числе процессов.
-   Эффективность использования вычислительных ресурсов невысока, что объясняется преобладанием времени накладных расходов над временем полезной работы.
-   Для больших объемов данных ожидается лучшая масштабируемость, так как время передачи будет доминировать над временем инициализации коммуникаций.

**Метрики:**  
Ускорение: `S(p) = T(1) / T(p)`  
Эффективность: `E(p) = S(p) / p × 100%`

#### Ограничения реализации

-   Алгоритм передает данные целиком на каждом шаге, что может создавать нагрузку на сеть при больших объемах.
-   Не реализовано перекрытие вычислений и коммуникаций (pipelining).
-   Отсутствует сегментация больших сообщений.

## 8. Выводы

Разработана и реализована операция коллективной рассылки данных на основе биномиального дерева с использованием только базовых MPI-операций. Алгоритм корректно работает с произвольным процессом-источником и поддерживает основные численные типы данных.

Сравнение с последовательной реализацией подтвердило идентичность результатов. Эксперименты показали, что для средних объемов данных параллельная версия начинает показывать преимущество при увеличении числа процессов. Полученная реализация может служить основой для других коллективных операций. Возможные направления оптимизации: использование асинхронных операций `MPI_Isend`/`MPI_Irecv`, сегментация больших сообщений, адаптивный выбор алгоритма в зависимости от размера данных.

## 9. Использованные источники

1.  Гергель В.П., Баркалов К.А., Мееров И.Б., Сысоев А.В. Параллельные вычисления. Технологии и численные методы: Учебное пособие в 4 томах. — Нижний Новгород: Изд-во ННГУ, 2013. — 1394 с.
2.  Корняков К.В., Мееров И.Б., Сиднев А.А., Сысоев А.В., Шишков А.В. Инструменты параллельного программирования в системах с общей памятью: Учебное пособие. — Нижний Новгород: Изд-во ННГУ, 2010. — 202 с.
3.  Гергель В.П. Современные языки и технологии параллельного программирования. — М.: Изд-во Московского университета, 2012. — 408 с.
4.  Документация Microsoft MPI. — URL: https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-allreduce-function (дата обращения: 21.11.2025).